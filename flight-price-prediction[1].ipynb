{"cells":[{"cell_type":"markdown","metadata":{},"source":["#  flight price prediction"]},{"cell_type":"markdown","metadata":{},"source":["![title](https://www.usnews.com/dims4/USNEWS/ed7b154/2147483647/crop/2000x1334%2B0%2B1/resize/970x647/quality/85/?url=http%3A%2F%2Fmedia.beam.usnews.com%2Fb7%2Ff4%2F847d373a4218bcbb9383ff501eaa%2F191004-flightapp-stock.jpg)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["import matplotlib .pyplot as plt\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{},"source":["# importing the data set"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data = pd.read_excel(\"../input/flight-fare-prediction-mh/Data_Train.xlsx\")\n","pd.set_option('display.max_columns', None)\n","train_data.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data.info()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data[\"Duration\"].value_counts()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data.dropna(inplace = True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data.isnull().sum()\n"]},{"cell_type":"markdown","metadata":{},"source":["# EDA"]},{"cell_type":"markdown","metadata":{},"source":["From description we can see that Date_of_Journey is a object data type,\\ Therefore, we have to convert this datatype into timestamp so as to use this column properly for prediction\n","\n","For this we require pandas to_datetime to convert object data type to datetime dtype.\n","\n","**.dt.day method will extract only day of that date**\\ **.dt.month method will extract only month of that date*"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data[\"Journey_day\"] = pd.to_datetime(train_data.Date_of_Journey, format=\"%d/%m/%Y\").dt.day\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data[\"Journey_month\"] = pd.to_datetime(train_data[\"Date_of_Journey\"], format = \"%d/%m/%Y\").dt.month\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Since we have converted Date_of_Journey column into integers, Now we can drop as it is of no use.\n","\n","train_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Departure time is when a plane leaves the gate. \n","# Similar to Date_of_Journey we can extract values from Dep_Time\n","\n","# Extracting Hours\n","train_data[\"Dep_hour\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.hour\n","\n","# Extracting Minutes\n","train_data[\"Dep_min\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.minute\n","\n","# Now we can drop Dep_Time as it is of no use\n","train_data.drop([\"Dep_Time\"], axis = 1, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Arrival time is when the plane pulls up to the gate.\n","# Similar to Date_of_Journey we can extract values from Arrival_Time\n","\n","# Extracting Hours\n","train_data[\"Arrival_hour\"] = pd.to_datetime(train_data.Arrival_Time).dt.hour\n","\n","# Extracting Minutes\n","train_data[\"Arrival_min\"] = pd.to_datetime(train_data.Arrival_Time).dt.minute\n","\n","# Now we can drop Arrival_Time as it is of no use\n","train_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Time taken by plane to reach destination is called Duration\n","# It is the differnce betwwen Departure Time and Arrival time\n","\n","\n","# Assigning and converting Duration column into list\n","duration = list(train_data[\"Duration\"])\n","\n","for i in range(len(duration)):\n","    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n","        if \"h\" in duration[i]:\n","            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n","        else:\n","            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n","\n","duration_hours = []\n","duration_mins = []\n","for i in range(len(duration)):\n","    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n","    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Adding duration_hours and duration_mins list to train_data dataframe\n","\n","train_data[\"Duration_hours\"] = duration_hours\n","train_data[\"Duration_mins\"] = duration_mins"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data.drop([\"Duration\"], axis = 1, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["# handling categorical data"]},{"cell_type":"markdown","metadata":{},"source":["ne can find many ways to handle categorical data. Some of them categorical data are,\n","\n","**Nominal data** --> data are not in any order --> **OneHotEncoder** is used in this case\n","**Ordinal data** --> data are in order --> **LabelEncoder** is used in this case"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data[\"Airline\"].value_counts()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# From graph we can see that Jet Airways Business have the highest Price.\n","# Apart from the first Airline almost all are having similar median\n","\n","# Airline vs Price\n","sns.catplot(y = \"Price\", x = \"Airline\", data = train_data.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 6, aspect = 3)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# As Airline is Nominal Categorical data we will perform OneHotEncoding\n","\n","Airline = train_data[[\"Airline\"]]\n","\n","Airline = pd.get_dummies(Airline, drop_first= True)\n","\n","Airline.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data[\"Source\"].value_counts()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Source vs Price\n","\n","sns.catplot(y = \"Price\", x = \"Source\", data = train_data.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 4, aspect = 3)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# As Source is Nominal Categorical data we will perform OneHotEncoding\n","\n","Source = train_data[[\"Source\"]]\n","\n","Source = pd.get_dummies(Source, drop_first= True)\n","\n","Source.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data[\"Destination\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# As Destination is Nominal Categorical data we will perform OneHotEncoding\n","\n","Destination = train_data[[\"Destination\"]]\n","\n","Destination = pd.get_dummies(Destination, drop_first = True)\n","\n","Destination.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data[\"Route\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Additional_Info contains almost 80% no_info\n","# Route and Total_Stops are related to each other\n","\n","train_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data[\"Total_Stops\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# As this is case of Ordinal Categorical type we perform LabelEncoder\n","# Here Values are assigned with corresponding keys\n","\n","train_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_data.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Concatenate dataframe --> train_data + Airline + Source + Destination\n","\n","data_train = pd.concat([train_data, Airline, Source, Destination], axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data_train.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data_train.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data_train.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data_train.shape\n"]},{"cell_type":"markdown","metadata":{},"source":["# test data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_data = pd.read_excel(\"../input/flight-fare-prediction-mh/Test_set.xlsx\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_data.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Preprocessing same as training data that we have done\n","\n","print(\"Test data Info\")\n","print(\"-\"*75)\n","print(test_data.info())\n","\n","print()\n","print()\n","\n","print(\"Null values :\")\n","print(\"-\"*75)\n","test_data.dropna(inplace = True)\n","print(test_data.isnull().sum())\n","\n","# EDA\n","\n","# Date_of_Journey\n","test_data[\"Journey_day\"] = pd.to_datetime(test_data.Date_of_Journey, format=\"%d/%m/%Y\").dt.day\n","test_data[\"Journey_month\"] = pd.to_datetime(test_data[\"Date_of_Journey\"], format = \"%d/%m/%Y\").dt.month\n","test_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n","\n","# Dep_Time\n","test_data[\"Dep_hour\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.hour\n","test_data[\"Dep_min\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.minute\n","test_data.drop([\"Dep_Time\"], axis = 1, inplace = True)\n","\n","# Arrival_Time\n","test_data[\"Arrival_hour\"] = pd.to_datetime(test_data.Arrival_Time).dt.hour\n","test_data[\"Arrival_min\"] = pd.to_datetime(test_data.Arrival_Time).dt.minute\n","test_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)\n","\n","# Duration\n","duration = list(test_data[\"Duration\"])\n","\n","for i in range(len(duration)):\n","    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n","        if \"h\" in duration[i]:\n","            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n","        else:\n","            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n","\n","duration_hours = []\n","duration_mins = []\n","for i in range(len(duration)):\n","    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n","    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration\n","\n","# Adding Duration column to test set\n","test_data[\"Duration_hours\"] = duration_hours\n","test_data[\"Duration_mins\"] = duration_mins\n","test_data.drop([\"Duration\"], axis = 1, inplace = True)\n","\n","\n","# Categorical data\n","\n","print(\"Airline\")\n","print(\"-\"*75)\n","print(test_data[\"Airline\"].value_counts())\n","Airline = pd.get_dummies(test_data[\"Airline\"], drop_first= True)\n","\n","print()\n","\n","print(\"Source\")\n","print(\"-\"*75)\n","print(test_data[\"Source\"].value_counts())\n","Source = pd.get_dummies(test_data[\"Source\"], drop_first= True)\n","\n","print()\n","\n","print(\"Destination\")\n","print(\"-\"*75)\n","print(test_data[\"Destination\"].value_counts())\n","Destination = pd.get_dummies(test_data[\"Destination\"], drop_first = True)\n","\n","# Additional_Info contains almost 80% no_info\n","# Route and Total_Stops are related to each other\n","test_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)\n","\n","# Replacing Total_Stops\n","test_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n","\n","# Concatenate dataframe --> test_data + Airline + Source + Destination\n","data_test = pd.concat([test_data, Airline, Source, Destination], axis = 1)\n","\n","data_test.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n","\n","print()\n","print()\n","\n","print(\"Shape of test data : \", data_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X = data_train.loc[:, ['Total_Stops', 'Journey_day', 'Journey_month', 'Dep_hour',\n","       'Dep_min', 'Arrival_hour', 'Arrival_min', 'Duration_hours',\n","       'Duration_mins', 'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',\n","       'Airline_Jet Airways', 'Airline_Jet Airways Business',\n","       'Airline_Multiple carriers',\n","       'Airline_Multiple carriers Premium economy', 'Airline_SpiceJet',\n","       'Airline_Trujet', 'Airline_Vistara', 'Airline_Vistara Premium economy',\n","       'Source_Chennai', 'Source_Delhi', 'Source_Kolkata', 'Source_Mumbai',\n","       'Destination_Cochin', 'Destination_Delhi', 'Destination_Hyderabad',\n","       'Destination_Kolkata', 'Destination_New Delhi']]\n","X.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y = data_train.iloc[:, 1]\n","y.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Finds correlation between Independent and dependent attributes\n","\n","plt.figure(figsize = (18,18))\n","sns.heatmap(train_data.corr(), annot = True, cmap = \"RdYlGn\")\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import sklearn "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Important feature using ExtraTreesRegressor\n","\n","from sklearn.ensemble import ExtraTreesRegressor\n","selection = ExtraTreesRegressor()\n","selection.fit(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(selection.feature_importances_)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#plot graph of feature importances for better visualization\n","\n","plt.figure(figsize = (12,8))\n","feat_importances = pd.Series(selection.feature_importances_, index=X.columns)\n","feat_importances.nlargest(20).plot(kind='barh')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["# fitting model using random forest"]},{"cell_type":"markdown","metadata":{},"source":["1. * > Split dataset into train and test set in order to prediction w.r.t X_test\n","2. * > If needed do scaling of data\n","3. * > Scaling is not done in Random forest\n","4. * > Import model\n","5. * > Fit the data\n","6. * > Predict w.r.t X_test\n","7. * > In regression check RSME Score\n","8. * > Plot graph"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.ensemble import RandomForestRegressor\n","reg_rf = RandomForestRegressor()\n","reg_rf.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y_pred = reg_rf.predict(X_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["reg_rf.score(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["reg_rf.score(X_test, y_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sns.distplot(y_test-y_pred)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.scatter(y_test, y_pred, alpha = 0.5)\n","plt.xlabel(\"y_test\")\n","plt.ylabel(\"y_pred\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn import metrics\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\n","print('MSE:', metrics.mean_squared_error(y_test, y_pred))\n","print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# RMSE/(max(DV)-min(DV))\n","\n","2090.5509/(max(y)-min(y))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metrics.r2_score(y_test, y_pred)"]},{"cell_type":"markdown","metadata":{},"source":["# hyperparameter tuning"]},{"cell_type":"markdown","metadata":{},"source":["* > Choose following method for hyperparameter tuning\n","* > RandomizedSearchCV --> Fast\n","* > GridSearchCV\n","* > Assign hyperparameters in form of dictionery\n","* > Fit the model\n","* > Check best paramters and best score"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import RandomizedSearchCV\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Randomized Search CV\n","\n","# Number of trees in random forest\n","n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n","# Number of features to consider at every split\n","max_features = ['auto', 'sqrt']\n","# Maximum number of levels in tree\n","max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n","# Minimum number of samples required to split a node\n","min_samples_split = [2, 5, 10, 15, 100]\n","# Minimum number of samples required at each leaf node\n","min_samples_leaf = [1, 2, 5, 10]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create the random grid\n","\n","random_grid = {'n_estimators': n_estimators,\n","               'max_features': max_features,\n","               'max_depth': max_depth,\n","               'min_samples_split': min_samples_split,\n","               'min_samples_leaf': min_samples_leaf}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Random search of parameters, using 5 fold cross validation, \n","# search across 100 different combinations\n","rf_random = RandomizedSearchCV(estimator = reg_rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["rf_random.fit(X_train,y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["rf_random.best_params_\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["prediction = rf_random.predict(X_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize = (8,8))\n","sns.distplot(y_test-prediction)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize = (8,8))\n","plt.scatter(y_test, prediction, alpha = 0.5)\n","plt.xlabel(\"y_test\")\n","plt.ylabel(\"y_pred\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print('MAE:', metrics.mean_absolute_error(y_test, prediction))\n","print('MSE:', metrics.mean_squared_error(y_test, prediction))\n","print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))"]},{"cell_type":"markdown","metadata":{},"source":["# save the model and reuse it again"]},{"cell_type":"markdown","metadata":{},"source":["thanku for reading this notebook hope you learn alot"]},{"cell_type":"markdown","metadata":{},"source":["if you like the notebook please comment and please upvote this"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":140442,"sourceId":330428,"sourceType":"datasetVersion"}],"dockerImageVersionId":29994,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
